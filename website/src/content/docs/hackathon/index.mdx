import AutoBeWaterfallDiagram from "../../../template/diagrams/AutoBeWaterfallDiagram.mdx";

## 1. Overview

![](/images/hackathon/poster-20250912.png)

Wrtn Technologies is hosting the 1st AutoBE Hackathon.

### Hackathon Information

**Event Details**
- **Participants**: 70 people (first-come, first-served)
- **Registration Period**: September 5 - 10, 2025
- **Event Schedule**: September 12 - 14, 2025 (64 hours)
  - **Start**: September 12, 08:00:00 (PDT, UTC-7)
  - **End**: September 14, 23:59:59 (PDT, UTC-7)

**Important Links**
- [Registration Form](https://forms.gle/8meMGEgKHTiQTrCT7)
- [Discord Channel](https://discord.gg/aMhRmzkqCx)


### AutoBE Resources

**Documentation & Code**
- [Guide Documents](https://autobe.dev/docs)
- [Github Repository](https://github.com/wrtnlabs/autobe)

**Example Applications Generated by AutoBE**
- [To Do List](https://github.com/wrtnlabs/autobe-example-todo)
- [Reddit Community](https://github.com/wrtnlabs/autobe-example-reddit) (fake)
- [Discussion Board](https://github.com/wrtnlabs/autobe-example-discussion)
- [E-Commerce Platform](https://github.com/wrtnlabs/autobe-example-ecommerce)


We want to ask backend developers: Can AI truly replace the work of backend developers? We seek to hear the answer to this question directly from developers working in the field.

AutoBE is an AI-based no-code platform that automatically generates backend applications through natural language conversations. When you discuss requirements with AutoBE's AI chatbot, AutoBE organizes them into a requirements specification, designs database schemas, defines APIs, writes test code, and ultimately implements a backend application that successfully builds. But is the generated code truly production-ready? Perhaps it's just seemingly plausible code fragments that differ from what users actually want?

This hackathon is designed precisely to validate this point. We expect developers with actual backend development experience to use AutoBE firsthand and provide honest, sharp evaluations from an expert perspective. Please tell us whether the backend applications generated by AutoBE truly match what you wanted. Your critical perspective and professional analysis will play a crucial role in developing AutoBE into a better tool.

### 1.1. Target Participants

**Who We're Looking For**

This hackathon is open to developers with **at least 1 year of practical backend development experience**.

We need developers who can:
- Not only write code but also distinguish between good and bad code
- Evaluate architectural pros and cons
- Provide realistic feedback based on actual service operation experience

**We Particularly Welcome**
- Those with deep insights into the limitations and possibilities of AI code generation tools
- Those who have considered how emerging development paradigms will impact the developer community
- Those who can objectively evaluate new technologies based on their expertise

### 1.2. Event Information

**Timeline**
- **Registration**: September 5 - 10, 2025
- **Hackathon Duration**: 64 hours (2 days and 16 hours)
  - **Start**: September 12, 08:00:00 (PDT, UTC-7)
  - **End**: September 14, 23:59:59 (PDT, UTC-7)



**Prize Pool: $6,400**
- **Best Review**: $2,000
- **Second Best Review**: $1,000
- **Participation Prize**: $50 (for all who provide meaningful feedback)


### 1.3. Next Hackathon Preview

**Why Only 70 Participants?**

This 1st hackathon is limited to **70 participants** on a first-come, first-served basis. The main reason for this limitation is AI token usage costs.

**Current Situation**
- AutoBE has focused on unit implementation and testing
- High-quality backend applications can be generated
- However, AI token usage optimization (RAG) is not yet implemented

**Token Usage Example**
- Large-scale e-commerce platform: ~150 million tokens
- Cost: Approximately **$300** per generation
- With hundreds of participants, costs would be prohibitive

> **Good News!**
>
> In the next hackathon, we'll introduce RAG technology to dramatically reduce token usage and prepare a grand event where many more developers can participate. This hackathon is the first step.

## 2. What is AutoBE?

**AutoBE is a vibe coding agent for building backend applications, enhanced with AI-friendly compilers.**

- [Github Repository](https://github.com/wrtnlabs/autobe)
- [Guide Documents](https://autobe.dev/docs)

**Key Innovation**

AutoBE is an AI-based no-code backend generation platform that creates fully functional production-grade backend applications from natural language requirements alone.

To solve the fundamental limitation of existing AI code generation tools - that generated code often doesn't compile or run - we introduced an innovative approach called **Compiler-in-the-Loop**.

**Core Achievement**

A vibe coding agent exclusively for backend application generation with **100% build success rate** (based on OpenAI GPT 4.1) - that's AutoBE.

### 2.1. How It Works

<AutoBeWaterfallDiagram />

AutoBE follows a **5-stage process** that reinterprets the traditional software engineering waterfall model for the AI era. Each stage is handled by specialized AI agents, with compilers performing real-time validation at every stage.

#### Stage 1: Analyze Agent
**Requirements Analysis**
- Systematically analyzes requirements entered in natural language
- Understands business logic, not just listing features
- Derives various user personas and defines permissions/roles
- Identifies and clarifies ambiguous or conflicting requirements

#### Stage 2: Prisma Agent
**Database Schema Design**
- Designs database schemas based on requirements
- Identifies relationships between entities
- Applies appropriate normalization
- Establishes indexing strategies
- Generates type-safe data models validated by Prisma compiler


#### Stage 3: Interface Agent
**API Design**
- Designs RESTful APIs
- Defines HTTP methods, URIs, and request/response formats
- Generates complete API documentation (OpenAPI 3.1)
- Must pass AutoBE-specific OpenAPI compiler


#### Stage 4: Test Agent
**E2E Test Generation**
- Writes E2E test code
- Creates test scenarios simulating actual user behavior
- Includes normal cases, edge cases, and error situations
- Test code must be executable and validated by test runner


#### Stage 5: Realize Agent
**Implementation**
- Implements actual backend code
- Based on NestJS framework
- Implements controller, service, and repository layers
- Handles dependency injection, middleware, and guards
- Must pass TypeScript compiler and NestJS builder


### 2.2. Technical Features

AutoBE's most distinctive feature is the integration of specialized compilers at each stage. They validate in real-time whether AI-generated code is syntactically correct, type-consistent, and actually executable.

**Compiler-in-the-Loop Process**
1. AI generates code
2. Compiler validates the code
3. If errors occur, AI receives feedback
4. AI modifies the code
5. Repeat until complete code is generated

#### AI-Specific Compilers

AutoBE's core competitive advantage lies in the AI-specific compilers independently developed for Prisma, Interface, and Test domains. Unlike general development tools, these compilers deeply understand and are optimized for AI characteristics:

**AI-specific Prisma Compiler**
- Goes beyond simply validating schema syntax
- Evaluates logical consistency and relationship appropriateness
- Preemptively detects circular references or duplicate relationships
- Provides AI-understandable feedback

**AI-specific Interface Compiler**
- Validates OpenAPI 3.1 spec compliance
- Checks RESTful principle adherence
- Ensures consistency between endpoints
- Detects missing authentication headers or error formats


**AI-specific Test Compiler**
- Analyzes whether test code performs meaningful validation
- Evaluates test coverage and edge case inclusion
- Assesses realism of test scenarios
- Suggests improvement directions to AI


> **Key Differentiator**
>
> While regular compilers simply say "there's an error," AutoBE's compilers provide detailed feedback on "why it's a problem" and "how to fix it" in a way AI can understand. This close collaboration is the secret to 100% build success rate.

#### AST-Based Code Generation

Another innovation of AutoBE is its structured code generation approach based on **AST (Abstract Syntax Tree)**.


**How It Works**
1. AI analyzes natural language requirements
2. AI generates data through function calling
3. Data follows predefined AST structures
4. AI "assembles" code rather than "writes" it
5. Generated AST is validated by compilers
6. AST is converted into actual usable code


**Check AST Structures on GitHub:**
- **Prisma Compiler**: [`AutoBePrisma.IApplication`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts)
- **Interface Compiler**: [`AutoBeOpenApi.IDocument`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts) 
- **Test Compiler**: [`AutoBeTest.IFunction`](https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts)

#### Technology Stack


**Modern, Proven Technologies**
- **Language**: TypeScript
- **Framework**: NestJS
- **ORM**: Prisma
- **Database**: PostgreSQL/SQLite

Generated code follows the same standards used in actual production environments.


### 2.3. Live Demonstration

We've prepared actual backend applications generated by AutoBE to prove its capabilities. These aren't prototypes or demos—they're fully functional production-grade applications created entirely through natural language conversations.

<br/>
<iframe
  src="https://www.youtube.com/embed/JNreQ0Rk94g"
  title="AutoBE Demonstration (Bullet-in Board System)"
  width="100%"
  style={{ aspectRatio: "16/9" }}
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  referrerPolicy="strict-origin-when-cross-origin"
  allowFullScreen
></iframe>

**Example Applications Generated by AutoBE**

From simple todo applications to complex e-commerce platforms, AutoBE has successfully generated various types of backend systems. Each application includes:
- Properly structured databases
- RESTful APIs
- Comprehensive test suites
- Production-ready code following best practices

#### Generated Applications

**1. [Discussion Board](https://github.com/wrtnlabs/autobe-example-bbs)**

**2. [To Do List](https://github.com/wrtnlabs/autobe-example-todo)**

**3. [Reddit Community](https://github.com/wrtnlabs/autobe-example-reddit)**

**4. [E-Commerce Platform](https://github.com/wrtnlabs/autobe-example-shopping)**
   - [Requirements Analysis Report](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/docs/analysis)
   - [Entity Relationship Diagram](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/docs/ERD.md) / [Prisma Schema](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/prisma/schema)
   - [API Controllers](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/controllers) / [DTO Structures](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/api/structures)
   - [E2E Test Functions](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/test/features/api)
   - [API Implementations](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/src/providers)
   - [AI Review](https://github.com/wrtnlabs/autobe-example-shopping/tree/main/AI_REVIEW.md)


#### How Simple Is It?

The process is remarkably simple. Creating a discussion board with AutoBE requires just five natural language commands. No coding knowledge, no technical jargon—just describe what you want:

**Example: Creating a Discussion Board**

1. I want to create a political/economic discussion board. Since I'm not familiar with programming, please write a requirements analysis report as you see fit.
2. Design the database schema.
3. Create the API interface specification.
4. Make the e2e test functions.
5. Implement API functions.

That's it. In about 70 minutes, you'll have a complete backend application ready to deploy. This isn't theory—it's how we generated all the examples above.

> [!TIP] 
>
> **For Hackathon Participants**
>
> Yes, these demo prompts are ridiculously simple. But during the hackathon, please don't just say "do everything by yourself!" 
>
> Actually discuss your requirements in detail with the AI. The better your input, the better your output will be.


## 3. Purpose of the Hackathon

**The Big Question**

AutoBE seems theoretically perfect. It has all the elements:
- Systematic processes
- Compiler validation
- Modern technology stack
- Code that compiles, runs, and passes tests

**But we still don't have an answer to one important question:**

### Is the backend application generated by AutoBE really what users wanted?

**Our Current Validation**

Until now, our development team has focused on whether each component of AutoBE works correctly:
- Compilers perform accurate validation
- Agents generate appropriate code
- The entire system operates stably

But these were all validations from a technical perspective.

**What We Need to Know**

In actual development fields, there are things as important as technical completeness:

- Is the generated code easy to maintain?
- Is the architecture scalable?
- Is performance optimization appropriate?
- Are there security vulnerabilities?
- Above all, is it "good code" from a developer's perspective?

> **Why Human Evaluation Matters**
>
> To answer these questions, we need evaluations from experts with actual backend development experience. While automated code review through AI is possible, we trust insights from human intuition and experience more.
>
> Especially evaluations from the perspective of **"What if I had to take over this code?"** can only be done by actual developers.

### 3.1. What We Want to Hear

**We Want Specific, Practical Feedback**

Not simple praise or criticism, but detailed analysis:

- **Requirements Specifications**: How do they compare to those used in actual projects?
- **Database Design**: Is it reasonable for long-term use?
- **API Design**: Does it properly follow RESTful principles?
- **Test Code**: Does it perform meaningful validation?
- **Implementation Code**: Does it have production-level quality?


**We're Also Curious About**

- How does AutoBE's generated code differ from what you would write yourself?
- What's better? What's lacking?
- What direction should we take for improvement?


> **Most Important**
>
> We expect your honest evaluation on whether AutoBE is truly a tool that can improve developer productivity or merely an interesting technical demo.

## 4. Eligibility and Requirements

### Minimum Requirements

**1. Backend Development Experience**
- At least **1 year** of practical experience
- Not just learning, but actually developing and operating real services

**2. Technology Stack Experience**

At least one of the following:
- Node.js (Express or NestJS)
- Java (Spring Boot)
- Python (Django or FastAPI)
- Similar backend frameworks

**3. Database Skills**
- Relational database design experience
- Beyond simple CRUD operations
- Experience with:
  - Designing relationships between tables
  - Establishing indexing strategies
  - Query optimization

**4. API Design Knowledge**
- Understanding of RESTful principles
- Experience applying them in real projects

### Language Requirements

**English Proficiency is Essential**
- All conversations with AutoBE are in English
- All generated code and documentation are in English
- You need:
  - Conversational ability to communicate naturally
  - Reading comprehension for technical documentation


### Technical Requirements

- Personal laptop or desktop computer
- Ability to download and run generated code locally
- Recommended to have installed:
  - Node.js
  - Git
  - Your preferred code editor


## 5. How to Participate

### 5.1. Registration

**Registration Form**

### https://forms.gle/8meMGEgKHTiQTrCT7

Those who wish to participate should submit an application through Google Forms.

**Required Information:**
- Basic personal information
- Backend development experience details


**Important Notice**

- Limited to **70 participants** on a first-come, first-served basis
- Will close early if 70 people register before the deadline
- Only the first 70 eligible applicants can participate

**Key Dates**

- **Application Deadline**: September 10, 2025
- **Notification**: September 11, 2025 (via email)
- **No applications accepted after the deadline**

### 5.2. Account Issuance and Preparation


**What You'll Receive (September 12)**

- Unique ID and password for AutoBE platform
- Available AI model information
- Simple guide on how to use AutoBE
- Technical support contact information

**Recommended Preparation**

Prepare your local development environment in advance:
- Node.js
- Git
- Your preferred code editor


### 5.3. Hackathon Process


**Start Time**
- **September 12, 8:00 AM PDT**
- Log into AutoBE platform with provided credentials

**Your Task**

Generate **2 backend applications** using different AI models:
1. `openai/gpt-4.1-mini`
2. `openai/gpt-4.1`


**Important Guidelines**

- Create applications with **different themes** for each model
- Example:
  - First model: Simple todo app
  - Second model: Complex e-commerce platform
- This helps evaluate each model's capabilities from various perspectives


**Document Everything**

During generation, carefully record:
- Conversation content with AutoBE
- Results from each stage
- Problems encountered and solutions
- Screenshots or logs

*These materials will be crucial for writing reviews later.*


### 5.4. Submission

**Submission Deadline**
- **September 14, 2025, 23:59:59 PDT**
- Submit detailed review documents

**Critical Requirement**

### You must submit a SEPARATE review for EACH generated backend application

- 2 applications = 2 individual reviews
- Do NOT combine multiple applications into one review
- Each application deserves its own thorough evaluation


**Where to Submit**

Post your reviews to AutoBE's GitHub Discussions:

https://github.com/wrtnlabs/autobe/discussions/categories/hackathon-20250912


**Review Requirements**

- No specific format required
- Should include sufficiently detailed analysis
- Don't just say "good" or "bad"
- Explain:
  - What parts were good/bad
  - In what ways
  - Why


## 6. Provided AI Models

### 6.1. `openai/gpt-4.1-mini`

![](/images/demonstrate/replay-openai-gpt4.1-mini.png)


**Overview**

A practical choice balancing performance and cost.

**Capabilities**
- Small to medium-sized backend applications
- ~20 tables
- ~150 API endpoints

**Best For**
- Community boards
- Blog platforms
- Project management tools


**Strengths**

- **Basic Features**: CRUD operations, user authentication, permission management, file uploads
- **Early Stages**: Excellent at requirements analysis and API design
- **Natural Language**: Converts requirements into well-structured specifications
- **Clean Output**: Consistently high-quality API designs

**Limitations**

- Occasional logical errors in complex business logic
- May fail to resolve compilation errors in E2E test code
- Inherent limitations of a lightweight model

*Note: These aren't technical defects in AutoBE, but model capacity limitations.*


**Why This Model First?**

1. **Educational Value**: Demonstrates the importance of model capacity in AI-powered code generation
2. **Cost Reality**: Using only powerful models would make this hackathon financially unfeasible
3. **Practical Workflow**: Many developers use this model for initial generation, then refine with AI assistants (Claude, Copilot)

This hybrid approach leverages AutoBE's structured generation while maintaining budget efficiency.


### 6.2. `openai/gpt-4.1`

![](/images/demonstrate/replay-openai-gpt4.1.png)


**Access Requirements**

> Available only after completing `openai/gpt-4.1-mini` review
>
> Once you complete your review of the mini model, you'll immediately gain access to the full-power `openai/gpt-4.1`


**Overview**

The most powerful AI model currently available.

**Capabilities**
- Large-scale enterprise-grade backend applications
- 500+ API endpoints
- 1,000+ test scenarios
- Complex business logic implementation

**Advanced Features**
- Real-time notification systems
- Complex permission systems
- Transaction processing
- Caching strategies


**The Magic**

### 100% Build Success Rate

- Every single backend application compiles perfectly
- All tests pass
- Genuinely production-ready code
- No compilation errors in test and realize stages

*The difference from the mini model is night and day!*


**Cost Reality**

- **Token Usage**: ~150 million tokens for e-commerce platform
- **Cost**: $300-400 per generation
- **Impact**: Limited hackathon participants
- **Solution**: Sequential model access

**Why Sequential Access?**
1. Helps manage costs
2. Provides perspective on model capacity impact
3. Free for hackathon participants once unlocked


### 6.3. `qwen/qwen3-235b-a22b-202507`

![](/images/demonstrate/replay-qwen3-235b-a22b.png)


**Optional - Just for Fun!**

> This model is NOT required for the hackathon. It's included purely for fun and for those curious about local LLM performance!

**What Is It?**
- Lightest open-source based model
- Requires only laptop-level resources
- A playground to explore open-source AI models

**Capabilities**

**Small-scale applications only:**
- 5-10 tables
- ~20 API endpoints
- Todo apps, memo applications, simple accounting
- Basic CRUD operations
- Simple business logic


**Limitations & Learning Opportunity**

**Significant limitations:**
- Struggles with complex requirements
- Often fails to resolve compilation errors
- Process interruptions common

**But that's what makes it interesting!**

Experience firsthand the performance gap between:
- Local open-source models
- Commercial cloud models

Who knows? You might be surprised by what it can (or can't) do!


## 7. Evaluation Criteria and Review Writing Guide

### 7.1. Requirements Analysis Stage Evaluation


**What to Evaluate**

**1. Understanding & Documentation**
- How accurately were your natural language requirements understood?
- Are relationships and priorities between features clearly defined?
- Is it more than just a feature list?

**2. User Personas & Roles**
- Are various user types considered?
- Are permissions and accessible features logically designed?
- Is the role hierarchy appropriate?

**3. Non-functional Requirements**
- Performance considerations
- Security measures
- Scalability planning

**4. Document Quality**
- Easy to read and understand?
- Any ambiguous or conflicting content?
- Sufficient detail to start development?


### 7.2. Database Design Evaluation

**What to Evaluate**

**1. Production-Readiness**
- Are table relationships logically valid?
- Any unnecessary duplications?
- Any circular references?

**2. Normalization Level**
- Over-normalized (complex joins)?
- Under-normalized (data integrity issues)?
- Appropriate balance?

**3. Keys & Indexing**
- Primary/foreign keys correctly set?
- Indexing strategy considers query performance?
- Missing indexes for common queries?

**4. Technical Details**
- Naming convention consistency
- Appropriate data types
- Default values and constraints
- Scalability for future changes

### 7.3. API Design Evaluation

**What to Evaluate**

**1. RESTful Principles**
- HTTP methods used meaningfully?
- URIs resource-centric?
- Status codes properly utilized?

**2. API Consistency**
- Similar endpoints follow patterns?
- Request/response formats unified?
- Error responses standardized?
- Common features (pagination, filtering, sorting) consistent?

**3. Documentation Quality**
- OpenAPI specs complete?
- Parameter descriptions sufficient?
- Clear examples provided?

**4. Security & Auth**
- Authentication/authorization reasonable?
- Sensitive data protected?
- API key management appropriate?

### 7.4. Test Code Evaluation

**What to Evaluate**

**1. Meaningful Validation**
- Verifies business logic, not just API calls?
- Tests actual functionality?
- Appropriate assertions?

**2. Scenario Completeness**
- Normal use cases covered?
- Exception situations tested?
- Edge cases included?
- User behavior patterns reflected?
- Important user journeys tested?

**3. Code Quality**
- Clear test function names?
- Appropriate test data setup?
- Specific assertions?
- Test independence guaranteed?
- Easy to debug when failing?

### 7.5. Implementation Code Evaluation

**What to Evaluate**

**1. Code Quality**
- Readable and understandable?
- Appropriate abstraction?
- Follows SOLID principles?
- Proper modularization?

**2. Architecture**
- Clear layer separation?
- Dependency injection utilized?
- Easy to extend/modify?
- Error handling systematic?
- Logging at appropriate levels?

**3. Performance**
- Efficient database queries?
- N+1 problems?
- Caching strategies?
- Resource optimization?

**4. Security & Types**
- SQL injection vulnerabilities?
- Input validation?
- TypeScript types properly used?
- No `any` type abuse?

### 7.6. Overall Evaluation

**Big Picture Questions**

**1. AutoBE Assessment**
- Overall strengths and weaknesses?
- Suitable project types?
- Unsuitable project types?
- Best use cases in actual development?

**2. Practical Impact**
- Actual development time reduction?
- Code quality level (junior/mid/senior)?
- Could you maintain this code?
- Would you use it in production?

**3. Improvement Suggestions**
- Specific parts to improve
- How to improve them
- Priority order
- Missing features

> **Remember**
>
> Don't just say "I wish it generated better code."
> Be specific about what should be improved and how.

## 8. Prizes and Benefits

### Total Prize Pool: $6,400

### 8.1. Grand Prize (1 person)

**Prize: $2,000**

We're looking for:
- Professional, balanced evaluation
- Specific, actionable improvement suggestions
- Quality over quantity
- Deep insights from real experience

### 8.2. Excellence Award (1 person)

**Prize: $1,000**

Selection based on:
- Review professionalism
- Quality of insights
- Constructive feedback

### 8.3. Participation Prize (All who qualify)

**Prize: $50**

**Requirements:**
- Generate projects using both required AI models
- Write detailed reviews for each project
- Include all required evaluation elements
- Meet minimum content requirements
- Provide meaningful feedback

### 8.4. Exclusion Conditions

**You will NOT receive prizes if:**

- Not providing even minimal review feedback
- Writing reviews using AI as proxy
- Not using both required models
- Writing perfunctory or insincere reviews
- Plagiarizing others' reviews


**AI-Assisted Reviews Not Allowed**

Why? The core purpose of this hackathon is to collect genuine feedback based on actual backend developers' experience.

AutoBE's development requires:
- Real opinions about inconveniences
- Practical improvement suggestions
- Utilization possibilities from experience

Formal AI-generated reviews don't serve this purpose. Participants found using AI for reviews will be excluded from evaluation.


### 8.5. Judging and Announcement


**Timeline**
- **Results Announcement**: **25.09.17(WED)**
  - Individual email
  - Official website (https://autobe.dev)

**Judges**
- AutoBE development team
- External experts

**Evaluation Criteria**
- Professionalism
- Specificity
- Practicality
- Balance

**Prize Payment**
- Within one week after announcement
- Requires international transfer capability
- Tax responsibility: Recipients
- Necessary documents provided

## 9. Disclaimer

### 9.1. Beta Version Limitations

AutoBE is currently in beta, still in pre-release development stage. Therefore it's not perfect and may have various problems and limitations. These are characteristics of the current development state, not bugs, so please understand and participate.

Generated code may not always be optimized and can sometimes be inefficient or unnecessarily complex. Also, compilation or runtime errors may occur in certain situations, and the process may stop without resolving them.

### 9.2. Use of Generated Code

We don't recommend using hackathon-generated code in actual production environments. Code generated by AutoBE hasn't undergone sufficient validation and may contain security vulnerabilities or performance issues.

If you decide to actually use generated code, please use it only after professional code review and security audit. Wrtn Technologies is not responsible for any issues arising from using AutoBE-generated code.

### 9.3. Open Source and Public Review Notice

AutoBE is an open-source project, and all hackathon reviews will be publicly posted on GitHub Discussions. Therefore, when using the AI chatbot during the hackathon, please be extremely careful not to input any sensitive personal information or business confidential information. Everything you discuss with the AI and all generated code will be part of your public review.

Remember: Your conversations, generated applications, and reviews will be visible to anyone on the internet. Plan your hackathon projects accordingly and avoid using real business ideas or proprietary information.